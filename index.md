---
layout: default
title: Home
nav_order: 1
description: "Catherine Arnett"
permalink: /
---

# Welcome

My name is [Catherine Arnett](https://catherinearnett.github.io/about). I'm currently a Linguistics PhD Candidate at UC San Diego and a research scientist at [PleIAs](https://pleias.fr/). My main [research interest](https://catherinearnett.github.io/research) is multilingual NLP. 

To contact me, you can email me at ccarnett [at] ucsd [dot] edu or find me on [Twitter](https://twitter.com/linguist_cat). You can find my work on [ResearchGate](https://www.researchgate.net/profile/Catherine-Arnett), [Orcid](https://orcid.org/0000-0003-0448-5415), and [Google Scholar](https://scholar.google.com/citations?user=bLS_8RAAAAAJ&hl=en). I theoretically post my code on [GitHub](https://github.com/catherinearnett). 

# News

*  Two of my papers were accepted to EMNLP:
    -  "[BPE Gets Picky: Efficient Vocabulary Refinement During Tokenizer Training](https://arxiv.org/pdf/2409.04599)", which was done with my colleagues at PleIAs
    -  "[When is Multilinguality a Curse? Language Modeling for 250 High- and Low-Resource Languages](https://arxiv.org/pdf/2311.09205)" which was done with my collaborators at UCSD CogSci
*  [Tyler Chang](https://tylerachang.github.io/) and I released [Goldfish Models](https://huggingface.co/goldfish-models), small comparable monolingual models for 350 languages. Check out the [Twitter thread overview](https://x.com/linguist_cat/status/1826267170952863885) of the release.
*  Our paper, [Revenge of the Fallen? Recurrent Models Match Transformers at Predicting Human Language Comprehension Metrics](https://arxiv.org/pdf/2404.19178) was accepted and will be presented at the first Conference on Language Modeling (COLM).
